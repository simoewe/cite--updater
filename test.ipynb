{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 400: {\"error\":\"Unrecognized or unsupported fields: [matchScore]\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-v] [-q] [--locals] [--durations N] [-f]\n",
      "                             [-c] [-b] [-k TESTNAMEPATTERNS]\n",
      "                             [tests ...]\n",
      "ipykernel_launcher.py: error: argument -f/--failfast: ignored explicit argument '/home/pranav/.local/share/jupyter/runtime/kernel-v2-911336vU4xFz4yd8fm.json'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import unittest\n",
    "\n",
    "s2_api_key = 'hYGM73pH6KsqgRrWoVXj4zy5wbFLEGuxfLVRIJ20'\n",
    "\n",
    "def search_papers_by_title(title, limit=10):\n",
    "    \"\"\"\n",
    "    Search for papers by title using the Semantic Scholar API.\n",
    "    \n",
    "    :param title: The title to search for\n",
    "    :param limit: Maximum number of results to return (default 10)\n",
    "    :return: List of paper data or None if the request fails\n",
    "    \"\"\"\n",
    "    url = \"http://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    \n",
    "    query_params = {\n",
    "        \"query\": title,\n",
    "        \"limit\": limit,\n",
    "        \"fields\": \"paperId,title,authors,year,abstract,citationCount,matchScore\"\n",
    "    }\n",
    "    \n",
    "    headers = {\"x-api-key\": s2_api_key}\n",
    "    \n",
    "    response = requests.get(url, params=query_params, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('data', [])\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "search_results = search_papers_by_title(\"Machine Learning\")\n",
    "if search_results:\n",
    "    for paper in search_results:\n",
    "        print(f\"Title: {paper['title']}\")\n",
    "        authors = ', '.join([author['name'] for author in paper.get('authors', [])])\n",
    "        print(f\"Authors: {authors}\")\n",
    "        print(f\"Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"Citation Count: {paper.get('citationCount', 'N/A')}\")\n",
    "        print(f\"Match Score: {paper.get('matchScore', 'N/A')}\")\n",
    "        print(f\"Abstract: {paper.get('abstract', 'N/A')[:200]}...\")  # Print first 200 characters of abstract\n",
    "        print(\"---\")\n",
    "\n",
    "# Test class\n",
    "class TestSemanticScholarAPI(unittest.TestCase):\n",
    "    def test_search_papers_by_title(self):\n",
    "        results = search_papers_by_title(\"Artificial Intelligence\", limit=5)\n",
    "        self.assertIsNotNone(results)\n",
    "        self.assertLessEqual(len(results), 5)\n",
    "        \n",
    "        for paper in results:\n",
    "            self.assertIn('title', paper)\n",
    "            self.assertIn('paperId', paper)\n",
    "            self.assertIn('authors', paper)\n",
    "            self.assertIn('matchScore', paper)\n",
    "    \n",
    "    def test_empty_search(self):\n",
    "        results = search_papers_by_title(\"\")\n",
    "        self.assertIsNone(results)\n",
    "    \n",
    "    def test_invalid_api_key(self):\n",
    "        global s2_api_key\n",
    "        original_key = s2_api_key\n",
    "        s2_api_key = \"invalid_key\"\n",
    "        \n",
    "        results = search_papers_by_title(\"Test\")\n",
    "        self.assertIsNone(results)\n",
    "        \n",
    "        s2_api_key = original_key\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'paperId': '649def34f8be52c8b66281af98ae884c09aef38b', 'title': 'Construction of the Literature Graph in Semantic Scholar', 'abstract': 'We describe a deployed scalable system for organizing published scientific literature into a heterogeneous graph to facilitate algorithmic manipulation and discovery. The resulting literature graph consists of more than 280M nodes, representing papers, authors, entities and various interactions between them (e.g., authorships, citations, entity mentions). We reduce literature graph construction into familiar NLP tasks (e.g., entity extraction and linking), point out research challenges due to differences from standard formulations of these tasks, and report empirical results for each task. The methods described in this paper are used to enable semantic features in www.semanticscholar.org.', 'year': 2018, 'citationCount': 376, 'authors': [{'authorId': '145585097', 'name': 'Bridger Waleed Ammar'}, {'authorId': '3458736', 'name': 'Dirk Groeneveld'}, {'authorId': '1857797', 'name': 'Chandra Bhagavatula'}, {'authorId': '46181066', 'name': 'Iz Beltagy'}, {'authorId': '46230609', 'name': 'Miles Crawford'}, {'authorId': '145612610', 'name': 'Doug Downey'}, {'authorId': '38092776', 'name': 'Jason Dunkelberger'}, {'authorId': '143718836', 'name': 'Ahmed Elgohary'}, {'authorId': '46411828', 'name': 'Sergey Feldman'}, {'authorId': '4480314', 'name': 'Vu A. Ha'}, {'authorId': '143967880', 'name': 'Rodney Michael Kinney'}, {'authorId': '41018147', 'name': 'Sebastian Kohlmeier'}, {'authorId': '46258841', 'name': 'Kyle Lo'}, {'authorId': '144240185', 'name': 'Tyler C. Murray'}, {'authorId': '46256862', 'name': 'Hsu-Han Ooi'}, {'authorId': '39139825', 'name': 'Matthew E. Peters'}, {'authorId': '39561369', 'name': 'Joanna L. Power'}, {'authorId': '46181683', 'name': 'Sam Skjonsberg'}, {'authorId': '31860505', 'name': 'Lucy Lu Wang'}, {'authorId': '46212260', 'name': 'Christopher Wilhelm'}, {'authorId': '2112339497', 'name': 'Zheng Yuan'}, {'authorId': '15292561', 'name': 'Madeleine van Zuylen'}, {'authorId': '1741101', 'name': 'Oren Etzioni'}], 'matchScore': 177.20428}]}\n",
      "No paper found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "s2_api_key = 'hYGM73pH6KsqgRrWoVXj4zy5wbFLEGuxfLVRIJ20'\n",
    "\n",
    "def match_paper_by_title(title):\n",
    "    \"\"\"\n",
    "    Match a single paper by title using the Semantic Scholar API.\n",
    "    \n",
    "    :param title: The title to search for\n",
    "    :return: Tuple of (paper data, match score) or (None, None) if the request fails\n",
    "    \"\"\"\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search/match\"\n",
    "    \n",
    "    query_params = {\n",
    "        \"query\": title,\n",
    "        \"fields\": \"paperId,title,authors,year,abstract,citationCount\"\n",
    "    }\n",
    "    \n",
    "    headers = {\"x-api-key\": s2_api_key}\n",
    "    \n",
    "    response = requests.get(url, params=query_params, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(data)\n",
    "        return data.get('paper'), data.get('matchScore')\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage\n",
    "paper, match_score = match_paper_by_title(\"Construction of the Literature Graph in Semantic Scholar\")\n",
    "if paper:\n",
    "    print(f\"Title: {paper['title']}\")\n",
    "    authors = ', '.join([author['name'] for author in paper.get('authors', [])])\n",
    "    print(f\"Authors: {authors}\")\n",
    "    print(f\"Year: {paper.get('year', 'N/A')}\")\n",
    "    print(f\"Citation Count: {paper.get('citationCount', 'N/A')}\")\n",
    "    print(f\"Match Score: {match_score}\")\n",
    "    print(f\"Abstract: {paper.get('abstract', 'N/A')[:200]}...\")  # Print first 200 characters of abstract\n",
    "else:\n",
    "    print(\"No paper found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Construction of the Literature Graph in Semantic Scholar\n",
      "Authors: Bridger Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu A. Ha, Rodney Michael Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler C. Murray, Hsu-Han Ooi, Matthew E. Peters, Joanna L. Power, Sam Skjonsberg, Lucy Lu Wang, Christopher Wilhelm, Zheng Yuan, Madeleine van Zuylen, Oren Etzioni\n",
      "Year: 2018\n",
      "Citation Count: 376\n",
      "Paper ID: 649def34f8be52c8b66281af98ae884c09aef38b\n",
      "Abstract: We describe a deployed scalable system for organizing published scientific literature into a heterogeneous graph to facilitate algorithmic manipulation and discovery. The resulting literature graph co...\n",
      "Match Score: N/A\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "s2_api_key = 'hYGM73pH6KsqgRrWoVXj4zy5wbFLEGuxfLVRIJ20'\n",
    "\n",
    "def search_papers_by_title(title):\n",
    "    \"\"\"\n",
    "    Search for papers by title using the Semantic Scholar API.\n",
    "    \n",
    "    :param title: The title to search for\n",
    "    :return: List of paper data or None if the request fails\n",
    "    \"\"\"\n",
    "    url = \"http://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    \n",
    "    query_params = {\n",
    "        \"query\": title,\n",
    "        \"limit\": 1,\n",
    "        \"fields\": \"paperId,title,authors,year,abstract,citationCount\"\n",
    "    }\n",
    "    \n",
    "    headers = {\"x-api-key\": s2_api_key}\n",
    "    \n",
    "    response = requests.get(url, params=query_params, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('data', [])\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "search_results = search_papers_by_title(\"Construction of the Literature Graph in Semantic Scholar\")\n",
    "if search_results:\n",
    "    for paper in search_results:\n",
    "        print(f\"Title: {paper['title']}\")\n",
    "        authors = ', '.join([author['name'] for author in paper.get('authors', [])])\n",
    "        print(f\"Authors: {authors}\")\n",
    "        print(f\"Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"Citation Count: {paper.get('citationCount', 'N/A')}\")\n",
    "        print(f\"Paper ID: {paper.get('paperId', 'N/A')}\")\n",
    "        print(f\"Abstract: {paper.get('abstract', 'N/A')[:200]}...\")  # Print first 200 characters of abstract\n",
    "        print(f\"Match Score: {paper.get('matchScore', 'N/A')}\")\n",
    "else:\n",
    "    print(\"No papers found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Construction of the Literature Graph in Semantic Scholar\n",
      "Authors: Bridger Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu A. Ha, Rodney Michael Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler C. Murray, Hsu-Han Ooi, Matthew E. Peters, Joanna L. Power, Sam Skjonsberg, Lucy Lu Wang, Christopher Wilhelm, Zheng Yuan, Madeleine van Zuylen, Oren Etzioni\n",
      "Year: 2018\n",
      "Citation Count: 376\n",
      "Paper ID: 649def34f8be52c8b66281af98ae884c09aef38b\n",
      "Abstract: We describe a deployed scalable system for organizing published scientific literature into a heterogeneous graph to facilitate algorithmic manipulation and discovery. The resulting literature graph co...\n",
      "Match Score: 174.45935\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "s2_api_key = 'hYGM73pH6KsqgRrWoVXj4zy5wbFLEGuxfLVRIJ20'\n",
    "\n",
    "def match_paper_by_title(title):\n",
    "    \"\"\"\n",
    "    Match a single paper by title using the Semantic Scholar API.\n",
    "    \n",
    "    :param title: The title to search for\n",
    "    :return: Tuple of (paper data, match score) or (None, None) if the request fails\n",
    "    \"\"\"\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search/match\"\n",
    "    \n",
    "    query_params = {\n",
    "        \"query\": title,\n",
    "        \"fields\": \"paperId,title,authors,year,abstract,citationCount\"\n",
    "    }\n",
    "    \n",
    "    headers = {\"x-api-key\": s2_api_key}\n",
    "    \n",
    "    response = requests.get(url, params=query_params, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if 'data' in data and len(data['data']) > 0:\n",
    "            paper = data['data'][0]\n",
    "            match_score = paper.pop('matchScore', None)\n",
    "            return paper, match_score\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage\n",
    "paper, match_score = match_paper_by_title(\"Construction of the Literature Graph in Semantic Scholar\")\n",
    "if paper:\n",
    "    print(f\"Title: {paper['title']}\")\n",
    "    authors = ', '.join([author['name'] for author in paper.get('authors', [])])\n",
    "    print(f\"Authors: {authors}\")\n",
    "    print(f\"Year: {paper.get('year', 'N/A')}\")\n",
    "    print(f\"Citation Count: {paper.get('citationCount', 'N/A')}\")\n",
    "    print(f\"Paper ID: {paper.get('paperId', 'N/A')}\")\n",
    "    print(f\"Abstract: {paper.get('abstract', 'N/A')[:200]}...\")  # Print first 200 characters of abstract\n",
    "    print(f\"Match Score: {match_score}\")\n",
    "else:\n",
    "    print(\"No paper found.\")\n",
    "\n",
    "# Add 1 second delay\n",
    "time.sleep(1)\n",
    "\n",
    "# Test function\n",
    "def test_match_paper_by_title():\n",
    "    paper, match_score = match_paper_by_title(\"Artificial Intelligence: A Modern Approach\")\n",
    "    assert paper is not None, \"Paper should not be None\"\n",
    "    assert match_score is not None, \"Match score should not be None\"\n",
    "    assert 'title' in paper, \"Paper should have a title\"\n",
    "    assert 'paperId' in paper, \"Paper should have a paperId\"\n",
    "    assert 'authors' in paper, \"Paper should have authors\"\n",
    "    assert 'year' in paper, \"Paper should have a year\"\n",
    "    assert 'citationCount' in paper, \"Paper should have a citation count\"\n",
    "    assert 'abstract' in paper, \"Paper should have an abstract\"\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_match_paper_by_title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2kenize: Tying Subword Sequences for Chinese Script Conversion\n",
      "{'title': '', 'first': 'Pranav', 'middle': '', 'last': 'A', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Isabelle', 'middle': '', 'last': 'Augenstein', 'suffix': '', 'nickname': ''}\n",
      "Faithfulness Tests for Natural Language Explanations\n",
      "{'title': '', 'first': 'Pepa', 'middle': '', 'last': 'Atanasova', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Oana-Maria', 'middle': '', 'last': 'Camburu', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Christina', 'middle': '', 'last': 'Lioma', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Thomas', 'middle': '', 'last': 'Lukasiewicz', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Jakob', 'middle': 'Grue', 'last': 'Simonsen', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Isabelle', 'middle': '', 'last': 'Augenstein', 'suffix': '', 'nickname': ''}\n",
      "The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models\n",
      "{'title': '', 'first': 'Noah', 'middle': 'Y.', 'last': 'Siegel', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Oana-Maria', 'middle': '', 'last': 'Camburu', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Nicolas', 'middle': '', 'last': 'Heess', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Maria', 'middle': '', 'last': 'Perez-Ortiz', 'suffix': '', 'nickname': ''}\n",
      "On Measuring Faithfulness or Self-consistency of Natural Language Explanations\n",
      "{'title': '', 'first': 'Letitia', 'middle': '', 'last': 'Parcalabescu', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Anette', 'middle': '', 'last': 'Frank', 'suffix': '', 'nickname': ''}\n",
      "Faithfulness Measurable Masked Language Models\n",
      "{'title': '', 'first': 'Andreas', 'middle': '', 'last': 'Madsen', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Siva', 'middle': '', 'last': 'Reddy', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Sarath', 'middle': '', 'last': 'Chandar', 'suffix': '', 'nickname': ''}\n",
      "NILE : Natural Language Inference with Faithful Natural Language Explanations\n",
      "{'title': '', 'first': 'Sawan', 'middle': '', 'last': 'Kumar', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Partha', 'middle': '', 'last': 'Talukdar', 'suffix': '', 'nickname': ''}\n",
      "Towards Faithful Model Explanation in NLP: A Survey\n",
      "{'title': '', 'first': 'Qing', 'middle': '', 'last': 'Lyu', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Marianna', 'middle': '', 'last': 'Apidianaki', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Chris', 'middle': '', 'last': 'Callison-Burch', 'suffix': '', 'nickname': ''}\n",
      "Faithful Model Evaluation for Model-Based Metrics\n",
      "{'title': '', 'first': 'Palash', 'middle': '', 'last': 'Goyal', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Qian', 'middle': '', 'last': 'Hu', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Rahul', 'middle': '', 'last': 'Gupta', 'suffix': '', 'nickname': ''}\n",
      "Incorporating Attribution Importance for Improving Faithfulness Metrics\n",
      "{'title': '', 'first': 'Zhixue', 'middle': '', 'last': 'Zhao', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Nikolaos', 'middle': '', 'last': 'Aletras', 'suffix': '', 'nickname': ''}\n",
      "FaithLM: Towards Faithful Explanations for Large Language Models\n",
      "{'title': '', 'first': 'Yu-Neng', 'middle': '', 'last': 'Chuang', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Guanchu', 'middle': '', 'last': 'Wang', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Chia-Yuan', 'middle': '', 'last': 'Chang', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Ruixiang', 'middle': '', 'last': 'Tang', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Shaochen', 'middle': '', 'last': 'Zhong', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Fan', 'middle': '', 'last': 'Yang', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Mengnan', 'middle': '', 'last': 'Du', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Xuanting', 'middle': '', 'last': 'Cai', 'suffix': '', 'nickname': ''}\n",
      "{'title': '', 'first': 'Xia', 'middle': '', 'last': 'Hu', 'suffix': '', 'nickname': ''}\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
    "search = arxiv.Search(\n",
    "  query = '(ti:Faithfulness Tests for Natural Language Explanations) OR (ti:Tying Subword Sequences for Chinese Script Conversion) OR (ti:Pixel-Based Language Modeling of Historical Documents) OR (ti:A Survey on the Use of Pre-trained Models for Natural Language Processing Tasks)',\n",
    "  max_results = 10,\n",
    "  sort_by = arxiv.SortCriterion.Relevance\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "\n",
    "# `results` is a generator; you can iterate over its elements one by one...\n",
    "for r in client.results(search):\n",
    "  print(r.title)\n",
    "  list_of_authors = [author.name for author in r.authors]\n",
    "  for author in list_of_authors:\n",
    "    name = HumanName(author)\n",
    "    print(name.as_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juan\n",
      "Q. Xavier\n",
      "de la Vega\n"
     ]
    }
   ],
   "source": [
    "from nameparser import HumanName\n",
    "name = HumanName(\"Dr. Juan Q. Xavier de la Vega III (Doc Vega)\")\n",
    "print(name.first)\n",
    "print(name.middle)\n",
    "print(name.last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n",
      "2024.lrec-main.320.pdf\n",
      "2020.acl-main.648.pdf\n",
      "2 files to process in current batch\n",
      "Processing of pdfs/2024.lrec-main.320.pdf failed with error 408 , None\n",
      "Processing of pdfs/2020.acl-main.648.pdf failed with error 408 , None\n"
     ]
    }
   ],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "\n",
    "client = GrobidClient(config_path=\"./config.json\")\n",
    "# Process references in pdfs folder, saving the output in the output folder\n",
    "client.process('processReferences', 'pdfs', output='output', consolidate_citations=False, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
